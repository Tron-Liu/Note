# 第五章 神经网络

## 5.4 全局最小与局部最小

**局部极小解** 是参数空间中的某个点，其邻域点的误差函数值 **均不小于** 该点的函数值  
**全局最小解** 指参数空间中所有点的误差函数值 **均不小于** 该点的误差函数值
  
显然，参数空间内梯度为零的点，只要其误差函数值小于邻点的误差函数值，就是局部极小点  
可能存在多个局部极小值，但却只会有一个全局最小值  
“全局最小”一定是“局部极小”，反之则不成立

## 5.5 其他常见网络

&emsp;RBF(Radial Basis Function, 径向基函数)网络 \[Broomhead and Lowe, 1988\] 是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。  
**径向基函数** 是某种沿径向对称的标量函数，通常定义为样本到数据中心之间径向距离（通常是欧式距离）的单调函数
&emsp;SOM(Self-Organizing Map, 自组织映射)网络 \[Kohonen, 1982\]是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间(通常为二维)，同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。  
&emsp;SOM的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元(best matching unit)。然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本之间的距离最小。这个过程不断迭代，直至收敛。
**级联相关网络** 是结构自适应网络的重要代表  
&emsp;**级联**是建立层次连接的层级结构。在开始训练时，网络只有输入层和输出层，处于最小拓扑结构；随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构。当新的隐层神经元加入时，其输入端连接权值是冻结固定的。  
&emsp;**相关**是指通过最大化新神经元的输出与网络误差之间的相关性(correlation)来训练相关的参数。  
与一般的前馈神经网络相比，级联相关网络无需设置网络层数、隐层神经元数目、且训练速度较快、但其在数据较小时易陷入过拟合。
“递归神经网络”(recurrent neural networks)允许网络中出现环状结构，从而可让一些神经元的输出反馈回来作为输入信号。这样的结构与信息反馈过程，使得网络在 t 时刻的输出状态不仅与 t 时刻的输入有关，还与 t-1 时刻的网络状态有关，从而能处理与时间有关的动态变化。  
&emsp;Elman网络 \[Elman, 1990\] 是最常用的递归神经网络之一，
