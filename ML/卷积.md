# 卷积

1. 卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互 (sparse interactions)、参数共享 (parameter sharing)、等变表示 (equivariant representations)。

## attention 与一维卷积的区别

[知乎链接](https://www.zhihu.com/question/288081659/answer/1222002868)

1. 一维卷积的感受野是有限的，注意力机制的感受野是无限的（全局的）
2. 一维卷积的连接强度（权重）是与输入无关的，注意力机制的连接强度是与输入有关的。

拿NLP中序列建模来做个例子，有几点区别：

1. Context Window：因为一维卷积需要指定窗口的大小，比如图中，就是每次只看3个词。而且attention，这里是self-attention，他的context window是“无限”的，无限是指序列的长度是多少，窗口的大小就是多少，因为attention权重的计算涉及到一个序列里面所有的词。如上图所示，句子有5个词，窗口大小就是5，所以一维卷积是“local”的，attention可以说是“global”的。
2. Time Complexity：这个其实也是刚才的窗口大小不同导致的，因为一维卷积只看k个词（窗口大小为k），如果序列长度为n，那么复杂度就是kn。而attention因为在每个位置，每个词的权重计算都要考虑到所有的词，所以复杂度就是n^2
3. Dynamic Weights: 和二维卷积一样，一维卷积的权重是不变的，就是不会随着在序列位置中的变化而改变，但是attention不一样，每个位置的权重都是不一样的，attention scores（weights）是由dot-product计算出来的，具体地, softmax(qk/d^1/2). 但由于self-attention复杂度高，对长序列建模效果没有那么好，所以对一维卷积有很多改进的工作，比如上图的Pay Less Attention with Lightweight and Dynamic Convolutions （ICLR19， FAIR）1。还有比较新的Time-aware Large Kernel Convolutions （TaLK）2。在NMT，LM等任务上的效果都能和attention-based模型扳手腕。基本上就从动态权重和动态窗口大小上面对一维卷积进行改进，而另一方面又有一些对attention的改进工作，比如将无限窗口限制一下，或者层次化一下，有太多文章这里就不赘述了。

## 一维卷积的不同类型

1. 一维 full 卷积
2. 一维 same 卷积
3. 一维 valid 卷积
4. 具备深度的一维卷积
5. 具备深度的张量与多个卷积核的卷积

[博客园链接](https://www.cnblogs.com/itmorn/p/11177439.html)
